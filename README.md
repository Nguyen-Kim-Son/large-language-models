# üó£Ô∏è M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn (Large Language Model - LLM)

Kh√≥a h·ªçc n√†y ƒë∆∞·ª£c chia th√†nh 02 ph·∫ßn:

1. üß© **C∆° b·∫£n v·ªÅ LLM** bao g·ªìm c√°c ki·∫øn th·ª©c v·ªÅ to√°n h·ªçc , Python, v√† m·∫°ng n∆° ron.
2. üßë‚Äçüî¨ **Nh√† nghi√™n c·ª©u LLM** t·∫≠p trung v√†o h·ªçc c√°ch l√†m th·∫ø n√†o ƒë·ªÉ x√¢y d·ª±ng ki·∫øn tr√∫c LLMs t·ªët nh·∫•t b·∫±ng c√°ch s·ª≠ d·ª•ng k·ªπ thu·∫≠t m·ªõi nh·∫•t. 

## üìù Notebooks

Danh s√°ch c√°c code m·∫´u v√† b√†i vi·∫øt li√™n quan ƒë·∫øn LLM 

### Tinh ch·ªânh (Fine-tuning)

| Notebook | M√¥ t·∫£ | B√†i vi·∫øt | Notebook |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Tinh ch·ªânh Llama 2 trong Google Colab | H∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc ƒë·ªÉ tinh ch·ªânh m√¥ h√¨nh Llama 2. | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| Tinh ch·ªânh LLMs v·ªõi Axolotl | H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ c√¥ng c·ª• tinh ch·ªânh hi·ªán ƒë·∫°i. | [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | W.I.P. |
| Tinh ch·ªânh m√¥ h√¨nh Mistral-7b model v·ªõi DPO | TƒÉng c∆∞·ªùng hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh tinh ch·ªânh ƒë∆∞·ª£c gi√°m s√°t v·ªõi DPO. | [Tweet](https://twitter.com/maximelabonne/status/1729936514107290022) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |

### L∆∞·ª£ng t·ª≠ h√≥a (Quantization)

| Notebook | M√¥ t·∫£ | B√†i vi·∫øt | Notebook |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Gi·ªõi thi·ªáu v·ªÅ L∆∞·ª£ng t·ª≠ h√≥a tr·ªçng s·ªë | T·ªëi ∆∞u h√≥a m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn b·∫±ng c√°ch s·ª≠ d·ª•ng l∆∞·ª£ng t·ª≠ h√≥a 8 bit. | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| 2. L∆∞·ª£ng t·ª≠ h√≥a LLM 4 bit b·∫±ng GPTQ | L∆∞·ª£ng t·ª≠ h√≥a m√¥ h√¨nh LLM c·ªßa b·∫°n v√† ch·∫°y tr√™n m√°y ng∆∞·ªùi d√πng. | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| 3. L∆∞·ª£ng t·ª≠ m√¥ h√¨nh Llama 2 v·ªõi GGUF v√† llama.cpp | L∆∞·ª£ng t·ª≠ m√¥ h√¨nh Llama 2 v·ªõi llama.cpp v√† t·∫£i GGUF l√™n HF Hub. | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: Th∆∞ vi·ªán nhanh nh·∫•t ƒë·ªÉ ch·∫°y LLM | L∆∞·ª£ng t·ª≠ h√≥a v√† ch·∫°y m√¥ h√¨nh EXL2¬†v√† t·∫£i ch√∫ng l√™n HF Hub. | [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |

### Kh√°c

| Notebook | M√¥ t·∫£ | B√†i vi·∫øt | Notebook |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| H·ª£p nh·∫•t LLM v·ªõi Mergekit | K·∫øt h·ª£p nhi·ªÅu LLM v√† t·∫°o m√¥ h√¨nh Frankenstein c·ªßa ri√™ng b·∫°n | [Tweet](https://twitter.com/maximelabonne/status/1740732104554807676) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| Chi·∫øn l∆∞·ª£c gi·∫£i m√£ trong c√°c m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn | H∆∞·ªõng d·∫´n t·∫°o vƒÉn b·∫£n t·ª´ t√¨m ki·∫øm ch√πm tia ƒë·∫øn l·∫•y m·∫´u h·∫°t nh√¢n | [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| Tr·ª±c quan h√≥a h√†m m·∫•t m√°t c·ªßa GPT-2 | S∆° ƒë·ªì 3D tr·ª±c quan h√†m m·∫•t m√°t d·ª±a tr√™n s·ª± nhi·ªÖu lo·∫°n tr·ªçng s·ªë. | [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |
| C·∫£i thi·ªán ChatGPT b·∫±ng S∆° ƒë·ªì tri th·ª©c | TƒÉng c∆∞·ªùng c√¢u tr·∫£ l·ªùi c·ªßa ChatGPT b·∫±ng bi·ªÉu ƒë·ªì tri th·ª©c. | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="images/colab.svg" alt="Open In Colab"></a> |

## üß© C∆° b·∫£n v·ªÅ LLM

![](images/roadmap_fundamentals.png)

### 1. To√°n h·ªçc cho h·ªçc m√°y

Tr∆∞·ªõc khi th√†nh th·∫°o machine learning, ƒëi·ªÅu quan tr·ªçng l√† ph·∫£i hi·ªÉu c√°c kh√°i ni·ªám to√°n h·ªçc c∆° b·∫£n h·ªó tr·ª£ c√°c thu·∫≠t to√°n n√†y.

- **ƒê·∫°i s·ªë tuy·∫øn t√≠nh (Linear Algebra)**: M√¥n n√†y r·∫•t quan tr·ªçng ƒë·ªÉ hi·ªÉu nhi·ªÅu thu·∫≠t to√°n, ƒë·∫∑c bi·ªát l√† nh·ªØng thu·∫≠t to√°n ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·ªçc s√¢u. C√°c kh√°i ni·ªám ch√≠nh bao g·ªìm vect∆°, ma tr·∫≠n, ƒë·ªãnh th·ª©c, gi√° tr·ªã ri√™ng v√† vect∆° ri√™ng, kh√¥ng gian vect∆° v√† c√°c ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh.
- **Gi·∫£i t√≠ch (Calculus)**: Nhi·ªÅu thu·∫≠t to√°n h·ªçc m√°y li√™n quan ƒë·∫øn vi·ªác t·ªëi ∆∞u h√≥a c√°c h√†m li√™n t·ª•c, ƒë√≤i h·ªèi s·ª± hi·ªÉu bi·∫øt v·ªÅ ƒë·∫°o h√†m, t√≠ch ph√¢n, gi·ªõi h·∫°n v√† chu·ªói. Ph√©p t√≠nh ƒëa bi·∫øn v√† kh√°i ni·ªám gradient c≈©ng r·∫•t quan tr·ªçng.
- **X√°c su·∫•t v√† Th·ªëng k√™**: ƒê√¢y l√† nh·ªØng th√¥ng tin quan tr·ªçng ƒë·ªÉ hi·ªÉu c√°ch c√°c m√¥ h√¨nh h·ªçc h·ªèi t·ª´ d·ªØ li·ªáu v√† ƒë∆∞a ra d·ª± ƒëo√°n. C√°c kh√°i ni·ªám ch√≠nh bao g·ªìm l√Ω thuy·∫øt x√°c su·∫•t, bi·∫øn ng·∫´u nhi√™n, ph√¢n b·ªë x√°c su·∫•t, k·ª≥ v·ªçng, ph∆∞∆°ng sai, hi·ªáp ph∆∞∆°ng sai, t∆∞∆°ng quan, ki·ªÉm tra gi·∫£ thuy·∫øt, kho·∫£ng tin c·∫≠y, ∆∞·ªõc t√≠nh kh·∫£ nƒÉng t·ªëi ƒëa v√† suy lu·∫≠n Bayes.

üìö T√†i nguy√™n h·ªçc t·∫≠p:

- [3Blue1Brown - The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): Chu·ªói video cung c·∫•p tr·ª±c quan h√¨nh h·ªçc cho c√°c kh√°i ni·ªám n√†y.
- [StatQuest with Josh Starmer - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): Cung c·∫•p nh·ªØng gi·∫£i th√≠ch ƒë∆°n gi·∫£n v√† r√µ r√†ng cho nhi·ªÅu kh√°i ni·ªám th·ªëng k√™.
- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html): M·ªôt c√°ch gi·∫£i th√≠ch tr·ª±c quan kh√°c v·ªÅ ƒë·∫°i s·ªë tuy·∫øn t√≠nh.
- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra): T·ªët cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu v√¨ n√≥ gi·∫£i th√≠ch c√°c kh√°i ni·ªám m·ªôt c√°ch r·∫•t tr·ª±c quan.
- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1): Kh√≥a h·ªçc t∆∞∆°ng t√°c bao g·ªìm t·∫•t c·∫£ nh·ªØng ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ t√≠nh to√°n.
- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability): T√†i li·ªáu h·ªçc t·∫≠p d·ªÖ hi·ªÉu. 
---

### 2. Python cho h·ªçc m√°y (Machine Learning)

Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh m·∫°nh m·∫Ω v√† linh ho·∫°t, ƒë·∫∑c bi·ªát t·ªët cho machine learning nh·ªù t√≠nh d·ªÖ ƒë·ªçc, t√≠nh nh·∫•t qu√°n v√† h·ªá sinh th√°i m·∫°nh m·∫Ω c·ªßa c√°c th∆∞ vi·ªán khoa h·ªçc d·ªØ li·ªáu.

- **Kh√°i ni·ªám c∆° b·∫£n v·ªÅ Python**: Vi·ªác hi·ªÉu bi·∫øt v·ªÅ c√∫ ph√°p c∆° b·∫£n, ki·ªÉu d·ªØ li·ªáu, x·ª≠ l√Ω l·ªói v√† l·∫≠p tr√¨nh h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng c·ªßa Python l√† r·∫•t quan tr·ªçng.
- **Th∆∞ vi·ªán khoa h·ªçc d·ªØ li·ªáu**: B·∫Øt bu·ªôc ph·∫£i l√†m quen v·ªõi NumPy cho c√°c ph√©p to√°n s·ªë, Pandas ƒë·ªÉ thao t√°c v√† ph√¢n t√≠ch d·ªØ li·ªáu, Matplotlib v√† Seaborn ƒë·ªÉ tr·ª±c quan h√≥a d·ªØ li·ªáu.
- **Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu**: Qu√° tr√¨nh n√†y bao g·ªìm vi·ªác chia t·ª∑ l·ªá v√† chu·∫©n h√≥a thu·ªôc t√≠nh, x·ª≠ l√Ω d·ªØ li·ªáu b·ªã thi·∫øu, ph√°t hi·ªán ngo·∫°i l·ªá, m√£ h√≥a d·ªØ li·ªáu theo ph√¢n lo·∫°i v√† chia d·ªØ li·ªáu th√†nh c√°c t·∫≠p hu·∫•n luy·ªán, x√°c th·ª±c v√† ki·ªÉm tra.
- **Th∆∞ vi·ªán cho h·ªçc m√°y**: Th√†nh th·∫°o Scikit-learn - th∆∞ vi·ªán cung c·∫•p nhi·ªÅu l·ª±a ch·ªçn thu·∫≠t to√°n h·ªçc c√≥ gi√°m s√°t v√† kh√¥ng gi√°m s√°t -l√† r·∫•t quan tr·ªçng. Hi·ªÉu c√°ch tri·ªÉn khai c√°c thu·∫≠t to√°n nh∆∞ h·ªìi quy tuy·∫øn t√≠nh, h·ªìi quy logistic, c√¢y quy·∫øt ƒë·ªãnh, r·ª´ng ng·∫´u nhi√™n, k-l√°ng gi·ªÅng g·∫ßn nh·∫•t (K-NN) v√† ph√¢n c·ª•m K-mean . C√°c k·ªπ thu·∫≠t gi·∫£m k√≠ch th∆∞·ªõc nh∆∞ PCA v√† t-SNE c≈©ng r·∫•t h·ªØu √≠ch ƒë·ªÉ hi·ªÉn th·ªã d·ªØ li·ªáu nhi·ªÅu chi·ªÅu.

üìö T√†i nguy√™n:

- [Real Python](https://realpython.com/): M·ªôt ngu·ªìn t√†i nguy√™n to√†n di·ªán v·ªõi c√°c b√†i vi·∫øt v√† h∆∞·ªõng d·∫´n cho c·∫£ kh√°i ni·ªám Python d√†nh cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu v√† n√¢ng cao.
- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw): Video d√†i cung c·∫•p ph·∫ßn gi·ªõi thi·ªáu ƒë·∫ßy ƒë·ªß v·ªÅ t·∫•t c·∫£ c√°c kh√°i ni·ªám c·ªët l√µi trong Python.
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Cu·ªën s√°ch k·ªπ thu·∫≠t s·ªë mi·ªÖn ph√≠ l√† ngu·ªìn t√†i nguy√™n tuy·ªát v·ªùi ƒë·ªÉ h·ªçc v·ªÅ panda, NumPy, matplotlib v√† Seaborn.
- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg): Gi·ªõi thi·ªáu th·ª±c t·∫ø v·ªÅ c√°c thu·∫≠t to√°n h·ªçc m√°y kh√°c nhau cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu.
- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120): Kh√≥a h·ªçc mi·ªÖn ph√≠ bao g·ªìm PCA v√† m·ªôt s·ªë kh√°i ni·ªám h·ªçc m√°y kh√°c.

---

### 3. M·∫°ng n∆° ron (Neural Networks)

M·∫°ng l∆∞·ªõi th·∫ßn kinh l√† m·ªôt ph·∫ßn c∆° b·∫£n c·ªßa nhi·ªÅu m√¥ h√¨nh h·ªçc m√°y, ƒë·∫∑c bi·ªát l√† trong lƒ©nh v·ª±c h·ªçc s√¢u. ƒê·ªÉ s·ª≠ d·ª•ng ch√∫ng m·ªôt c√°ch hi·ªáu qu·∫£, c·∫ßn ph·∫£i c√≥ s·ª± hi·ªÉu bi·∫øt to√†n di·ªán v·ªÅ thi·∫øt k·∫ø v√† c∆° ch·∫ø c·ªßa ch√∫ng.

- **C∆° b·∫£n**: G·ªìm vi·ªác hi·ªÉu c·∫•u tr√∫c c·ªßa m·∫°ng l∆∞·ªõi th·∫ßn kinh nh∆∞ c√°c l·ªõp, tr·ªçng s·ªë, ƒë·ªô l·ªách, h√†m k√≠ch ho·∫°t (sigmoid, tanh, ReLU, v.v.)
- **Hu·∫•n luy·ªán v√† t·ªëi ∆∞u**: L√†m quen v·ªõi lan truy·ªÅn ng∆∞·ª£c v√† c√°c lo·∫°i h√†m m·∫•t m√°t kh√°c nhau, nh∆∞ L·ªói b√¨nh ph∆∞∆°ng trung b√¨nh (MSE) v√† Entropy ch√©o. Hi·ªÉu c√°c thu·∫≠t to√°n t·ªëi ∆∞u h√≥a kh√°c nhau nh∆∞ Gi·∫£m d·∫ßn ƒë·ªô d·ªëc, Gi·∫£m d·∫ßn ƒë·ªô d·ªëc ng·∫´u nhi√™n, RMSprop v√† Adam.
- **Overfitting**: Hi·ªÉu kh√°i ni·ªám v·ªÅ overfitting (trong ƒë√≥ m·ªôt m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët tr√™n d·ªØ li·ªáu hu·∫•n luy·ªán nh∆∞ng k√©m tr√™n d·ªØ li·ªáu kh√¥ng nh√¨n th·∫•y) v√† c√°c k·ªπ thu·∫≠t ch√≠nh quy h√≥a kh√°c nhau ƒë·ªÉ ngƒÉn ch·∫∑n overfitting. C√°c k·ªπ thu·∫≠t bao g·ªìm dropout, chu·∫©n h√≥a L1/L2, d·ª´ng s·ªõm v√† tƒÉng c∆∞·ªùng d·ªØ li·ªáu.
- **Tri·ªÉn khai Perceptron ƒëa l·ªõp (MLP)**: X√¢y d·ª±ng MLP, c√≤n ƒë∆∞·ª£c g·ªçi l√† m·∫°ng ƒë∆∞·ª£c k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß, s·ª≠ d·ª•ng PyTorch.

üìö T√†i nguy√™n:

- [3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk): Video n√†y ƒë∆∞a ra l·ªùi gi·∫£i th√≠ch tr·ª±c quan v·ªÅ m·∫°ng l∆∞·ªõi th·∫ßn kinh v√† ho·∫°t ƒë·ªông b√™n trong c·ªßa ch√∫ng.
- [freeCodeCamp - Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c): Video n√†y gi·ªõi thi·ªáu m·ªôt c√°ch hi·ªáu qu·∫£ t·∫•t c·∫£ c√°c kh√°i ni·ªám quan tr·ªçng nh·∫•t trong h·ªçc s√¢u.
- [Fast.ai - Practical Deep Learning](https://course.fast.ai/): Kh√≥a h·ªçc mi·ªÖn ph√≠ ƒë∆∞·ª£c thi·∫øt k·∫ø d√†nh cho nh·ªØng ng∆∞·ªùi c√≥ kinh nghi·ªám vi·∫øt m√£ mu·ªën t√¨m hi·ªÉu v·ªÅ deep learning.
- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): lo·∫°t video d√†nh cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu t√¨m hi·ªÉu v·ªÅ PyTorch.

---

### 4. X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP)

NLP l√† m·ªôt nh√°nh c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o gi√∫p thu h·∫πp kho·∫£ng c√°ch gi·ªØa ng√¥n ng·ªØ con ng∆∞·ªùi v√† s·ª± hi·ªÉu bi·∫øt c·ªßa m√°y m√≥c. T·ª´ x·ª≠ l√Ω vƒÉn b·∫£n ƒë∆°n gi·∫£n ƒë·∫øn hi·ªÉu c√°c s·∫Øc th√°i ng√¥n ng·ªØ, NLP ƒë√≥ng m·ªôt vai tr√≤ quan tr·ªçng trong nhi·ªÅu ·ª©ng d·ª•ng nh∆∞ d·ªãch thu·∫≠t, ph√¢n t√≠ch t√¨nh c·∫£m, chatbot, v.v.

- **Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n**: T√¨m hi·ªÉu c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n kh√°c nhau nh∆∞ m√£ h√≥a (chia vƒÉn b·∫£n th√†nh c√°c t·ª´ ho·∫∑c c√¢u), r√∫t g·ªçn t·ª´ g·ªëc (r√∫t g·ªçn c√°c t·ª´ v·ªÅ d·∫°ng g·ªëc), t·ª´ v·ª±ng h√≥a (t∆∞∆°ng t·ª± nh∆∞ t√°ch g·ªëc nh∆∞ng c√≥ t√≠nh ƒë·∫øn ng·ªØ c·∫£nh), lo·∫°i b·ªè t·ª´, v.v. .
- **K·ªπ thu·∫≠t tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng**: L√†m quen v·ªõi c√°c k·ªπ thu·∫≠t chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n sang ƒë·ªãnh d·∫°ng m√† thu·∫≠t to√°n m√°y h·ªçc c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c. C√°c ph∆∞∆°ng ph√°p ch√≠nh bao g·ªìm T√∫i t·ª´ (BoW), T·∫ßn s·ªë t√†i li·ªáu ngh·ªãch ƒë·∫£o t·∫ßn s·ªë thu·∫≠t ng·ªØ (TF-IDF) v√† n-gram.
- **Nh√∫ng t·ª´**: Nh√∫ng t·ª´ l√† m·ªôt ki·ªÉu tr√¨nh b√†y t·ª´ cho ph√©p c√°c t·ª´ c√≥ nghƒ©a t∆∞∆°ng t·ª± c√≥ c√°ch tr√¨nh b√†y t∆∞∆°ng t·ª±. C√°c ph∆∞∆°ng th·ª©c ch√≠nh bao g·ªìm Word2Vec, GloVe v√† FastText.
- **M·∫°ng th·∫ßn kinh h·ªìi quy (RNN)**: Hi·ªÉu ho·∫°t ƒë·ªông c·ªßa RNN, m·ªôt lo·∫°i m·∫°ng th·∫ßn kinh ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ho·∫°t ƒë·ªông v·ªõi d·ªØ li·ªáu chu·ªói. Kh√°m ph√° LSTM v√† GRU, hai bi·∫øn th·ªÉ RNN c√≥ kh·∫£ nƒÉng h·ªçc c√°c ph·∫ßn ph·ª• thu·ªôc l√¢u d√†i.
  
üìö T√†i nguy√™n:

- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): H∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß v·ªÅ th∆∞ vi·ªán spaCy cho c√°c t√°c v·ª• NLP trong Python.
- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing): M·ªôt notebooks v√† t√†i nguy√™n ƒë·ªÉ gi·∫£i th√≠ch th·ª±c t·∫ø v·ªÅ NLP trong Python.
- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/): T√†i li·ªáu tham kh·∫£o t·ªët ƒë·ªÉ hi·ªÉu ki·∫øn tr√∫c n·ªïi ti·∫øng Word2Vec .
- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): Tri·ªÉn khai th·ª±c t·∫ø v√† ƒë∆°n gi·∫£n c√°c m√¥ h√¨nh RNN, LSTM v√† GRU trong PyTorch.
- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): B√†i vi·∫øt mang t√≠nh l√Ω thuy·∫øt nhi·ªÅu h∆°n v·ªÅ m·∫°ng LSTM.

## üßë‚Äçüî¨ **Nh√† nghi√™n c·ª©u LLM**

![](images/roadmap_scientist.png)

### 1. Ki·∫øn tr√∫c c·ªßa LLM

M·∫∑c d√π kh√¥ng c·∫ßn ph·∫£i c√≥ ki·∫øn th·ª©c chuy√™n s√¢u v·ªÅ ki·∫øn tr√∫c Transformer nh∆∞ng ƒëi·ªÅu quan tr·ªçng l√† ph·∫£i hi·ªÉu r√µ v·ªÅ ƒë·∫ßu v√†o (m√£ th√¥ng b√°o) v√† ƒë·∫ßu ra (logits) c·ªßa n√≥. C∆° ch·∫ø ch√∫ √Ω c∆° b·∫£n l√† m·ªôt th√†nh ph·∫ßn quan tr·ªçng kh√°c c·∫ßn ph·∫£i th√†nh th·∫°o, v√¨ c√°c phi√™n b·∫£n c·∫£i ti·∫øn c·ªßa n√≥ s·∫Ω ƒë∆∞·ª£c gi·ªõi thi·ªáu sau n√†y.

* **G√≥c nh√¨n t·ªïng quan**: Xem l·∫°i ki·∫øn tr√∫c Transformer b·ªô m√£ h√≥a-gi·∫£i m√£ v√† c·ª• th·ªÉ h∆°n l√† ki·∫øn tr√∫c GPT ch·ªâ d√†nh cho b·ªô gi·∫£i m√£, ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m·ªçi LLM hi·ªán ƒë·∫°i.
* **M√£ th√¥ng b√°o (Tokenization)**: Hi·ªÉu c√°ch chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu vƒÉn b·∫£n th√¥ sang ƒë·ªãnh d·∫°ng m√† m√¥ h√¨nh c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c, bao g·ªìm vi·ªác chia vƒÉn b·∫£n th√†nh c√°c m√£ th√¥ng b√°o (th∆∞·ªùng l√† t·ª´ ho·∫∑c t·ª´ ph·ª•).
* **C∆° ch·∫ø ch√∫ √Ω (Attention mechanisms)**: N·∫Øm b·∫Øt l√Ω thuy·∫øt ƒë·∫±ng sau c√°c c∆° ch·∫ø ch√∫ √Ω, bao g·ªìm c·∫£ s·ª± t·ª± ch√∫ √Ω v√† s·ª± ch√∫ √Ω c·ªßa t√≠ch v√¥ h∆∞·ªõng theo t·ª∑ l·ªá (scaled dot product), cho ph√©p m√¥ h√¨nh t·∫≠p trung v√†o c√°c ph·∫ßn kh√°c nhau c·ªßa ƒë·∫ßu v√†o khi t·∫°o ƒë·∫ßu ra.
* **T·∫°o vƒÉn b·∫£n**: T√¨m hi·ªÉu v·ªÅ c√°c c√°ch kh√°c nhau m√† m√¥ h√¨nh c√≥ th·ªÉ t·∫°o ra chu·ªói ƒë·∫ßu ra. C√°c chi·∫øn l∆∞·ª£c ph·ªï bi·∫øn bao g·ªìm gi·∫£i m√£ tham lam, t√¨m ki·∫øm ch√πm tia, l·∫•y m·∫´u top-k v√† l·∫•y m·∫´u h·∫°t nh√¢n.

üìö **T√†i li·ªáu tham kh·∫£o**:
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) c·ªßa Jay Alammar: Gi·∫£i th√≠ch tr·ª±c quan v·ªÅ m√¥ h√¨nh Transformer.
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) c·ªßa Jay Alammar: Th·∫≠m ch√≠ c√≤n quan tr·ªçng h∆°n b√†i vi·∫øt tr∆∞·ªõc, n√≥ t·∫≠p trung v√†o ki·∫øn tr√∫c GPT, r·∫•t gi·ªëng v·ªõi ki·∫øn tr√∫c c·ªßa Llama.
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) c·ªßa Andrej Karpathy: M·ªôt video YouTube d√†i 2 gi·ªù ƒë·ªÉ tri·ªÉn khai l·∫°i GPT t·ª´ ƒë·∫ßu (d√†nh cho l·∫≠p tr√¨nh vi√™n).
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) c·ªßa Lilian Weng: Gi·ªõi thi·ªáu nhu c·∫ßu ƒë∆∞·ª£c ch√∫ √Ω theo c√°ch trang tr·ªçng h∆°n.
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): Cung c·∫•p m√£ v√† ph·∫ßn gi·ªõi thi·ªáu tr·ª±c quan v·ªÅ c√°c chi·∫øn l∆∞·ª£c gi·∫£i m√£ kh√°c nhau ƒë·ªÉ t·∫°o vƒÉn b·∫£n.

---
### 2. X√¢y d·ª±ng t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n

M·∫∑c d√π c√≥ th·ªÉ d·ªÖ d√†ng ƒë·ªÉ t√¨m th·∫•y d·ªØ li·ªáu th√¥ t·ª´ Wikipedia v√† c√°c trang web kh√°c, nh∆∞ng vi·ªác thu th·∫≠p c√°c c·∫∑p h∆∞·ªõng d·∫´n v√† c√¢u tr·∫£ l·ªùi m·ªôt c√°ch t·ª± nhi√™n l·∫°i r·∫•t kh√≥ khƒÉn. Gi·ªëng nh∆∞ trong h·ªçc m√°y truy·ªÅn th·ªëng, ch·∫•t l∆∞·ª£ng c·ªßa t·∫≠p d·ªØ li·ªáu s·∫Ω ·∫£nh h∆∞·ªüng tr·ª±c ti·∫øp ƒë·∫øn ch·∫•t l∆∞·ª£ng c·ªßa m√¥ h√¨nh, ƒë√≥ l√† l√Ω do t·∫°i sao n√≥ c√≥ th·ªÉ l√† th√†nh ph·∫ßn quan tr·ªçng nh·∫•t trong qu√° tr√¨nh tinh ch·ªânh.

* **[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-like dataset**: T·∫°o d·ªØ li·ªáu t·ªïng h·ª£p t·ª´ ƒë·∫ßu b·∫±ng API OpenAI (GPT). B·∫°n c√≥ th·ªÉ ch·ªâ ƒë·ªãnh seed v√† l·ªùi nh·∫Øc h·ªá th·ªëng ƒë·ªÉ t·∫°o t·∫≠p d·ªØ li·ªáu ƒëa d·∫°ng.
* **K·ªπ thu·∫≠t n√¢ng cao**: T√¨m hi·ªÉu c√°ch c·∫£i thi·ªán c√°c t·∫≠p d·ªØ li·ªáu hi·ªán c√≥ v·ªõi [Evol-Instruct](https://arxiv.org/abs/2304.12244), c√°ch t·∫°o d·ªØ li·ªáu t·ªïng h·ª£p ch·∫•t l∆∞·ª£ng cao nh∆∞ trong [Orca](https ://arxiv.org/abs/2306.02707) v√† c√°c b√†i vi·∫øt [phi-1](https://arxiv.org/abs/2306.11644).
* **L·ªçc d·ªØ li·ªáu**: C√°c k·ªπ thu·∫≠t truy·ªÅn th·ªëng li√™n quan ƒë·∫øn bi·ªÉu th·ª©c ch√≠nh quy, lo·∫°i b·ªè c√°c n·ªôi dung g·∫ßn tr√πng l·∫∑p, t·∫≠p trung v√†o c√°c c√¢u tr·∫£ l·ªùi c√≥ s·ªë l∆∞·ª£ng m√£ th√¥ng b√°o cao, v.v.
* **M·∫´u l·ªùi nh·∫Øc**: Kh√¥ng c√≥ c√°ch ti√™u chu·∫©n th·ª±c s·ª± n√†o ƒë·ªÉ ƒë·ªãnh d·∫°ng h∆∞·ªõng d·∫´n v√† c√¢u tr·∫£ l·ªùi, ƒë√≥ l√† l√Ω do t·∫°i sao ƒëi·ªÅu quan tr·ªçng l√† ph·∫£i bi·∫øt v·ªÅ c√°c m·∫´u tr√≤ chuy·ªán kh√°c nhau, ch·∫≥ng h·∫°n nh∆∞ [ChatML](https://learn.microsoft.com/en- us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-lingu-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca .html), v.v.

üìö **T√†i li·ªáu tham kh·∫£o**:
* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) c·ªßa Thomas Capelle: Kh√°m ph√° b·ªô d·ªØ li·ªáu Alpaca v√† Alpaca-GPT4 v√† c√°ch ƒë·ªãnh d·∫°ng ch√∫ng.
* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) c·ªßa Solano Todeschini: H∆∞·ªõng d·∫´n c√°ch t·∫°o t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n t·ªïng h·ª£p b·∫±ng GPT-4.
* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) c·ªßa Kshitiz Sahay: S·ª≠ d·ª•ng GPT 3.5 ƒë·ªÉ t·∫°o t·∫≠p d·ªØ li·ªáu h∆∞·ªõng d·∫´n nh·∫±m tinh ch·ªânh Llama 2 ƒë·ªÉ ph√¢n lo·∫°i tin t·ª©c.
* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): Notebooks ch·ª©a m·ªôt s·ªë k·ªπ thu·∫≠t ƒë·ªÉ l·ªçc t·∫≠p d·ªØ li·ªáu v√† t·∫£i k·∫øt qu·∫£ l√™n.
* [Chat Template](https://huggingface.co/blog/chat-templates) c·ªßa Matthew Carrigan: Trang c·ªßa Hugging Face v·ªÅ c√°c m·∫´u l·ªùi nh·∫Øc

---
### 3. M√¥ h√¨nh hu·∫•n luy·ªán tr∆∞·ªõc (Pre-training models)

Hu·∫•n luy·ªán tr∆∞·ªõc l√† m·ªôt qu√° tr√¨nh r·∫•t d√†i v√† t·ªën k√©m, ƒë√≥ l√† l√Ω do t·∫°i sao ƒë√¢y kh√¥ng ph·∫£i l√† tr·ªçng t√¢m c·ªßa kh√≥a h·ªçc n√†y. S·∫Ω r·∫•t t·ªët n·∫øu b·∫°n hi·ªÉu bi·∫øt ·ªü m·ª©c ƒë·ªô n√†o ƒë√≥ v·ªÅ nh·ªØng g√¨ x·∫£y ra trong qu√° tr√¨nh hu·∫•n luy·ªán tr∆∞·ªõc, nh∆∞ng kh√¥ng c·∫ßn ph·∫£i c√≥ kinh nghi·ªám th·ª±c h√†nh.

* **ƒê∆∞·ªùng ·ªëng d·ªØ li·ªáu (Data pipeline)**: Qu√° tr√¨nh hu·∫•n luy·ªán tr∆∞·ªõc y√™u c·∫ßu c√°c b·ªô d·ªØ li·ªáu kh·ªïng l·ªì (v√≠ d·ª•: [Llama 2](https://arxiv.org/abs/2307.09288) ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n 2 ngh√¨n t·ª∑ m√£ th√¥ng b√°o) c·∫ßn ƒë∆∞·ª£c l·ªçc, m√£ h√≥a v√† ƒë·ªëi chi·∫øu v·ªõi m·ªôt t·ª´ v·ª±ng ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc.
* **M√¥ h√¨nh h√≥a ng√¥n ng·ªØ nh√¢n qu·∫£**: T√¨m hi·ªÉu s·ª± kh√°c bi·ªát gi·ªØa m√¥ h√¨nh h√≥a ng√¥n ng·ªØ nh√¢n qu·∫£ v√† ng√¥n ng·ªØ m·∫∑t n·∫°, c≈©ng nh∆∞ h√†m m·∫•t m√°t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong tr∆∞·ªùng h·ª£p n√†y. ƒê·ªÉ hu·∫•n luy·ªán tr∆∞·ªõc hi·ªáu qu·∫£, h√£y t√¨m hi·ªÉu th√™m v·ªÅ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM).
* **Lu·∫≠t chia t·ª∑ l·ªá**: [Lu·∫≠t chia t·ª∑ l·ªá](https://arxiv.org/pdf/2001.08361.pdf) m√¥ t·∫£ hi·ªáu su·∫•t d·ª± ki·∫øn c·ªßa m√¥ h√¨nh d·ª±a tr√™n k√≠ch th∆∞·ªõc m√¥ h√¨nh, k√≠ch th∆∞·ªõc t·∫≠p d·ªØ li·ªáu v√† l∆∞·ª£ng ƒëi·ªán to√°n ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√†o t·∫°o .


üìö **T√†i li·ªáu tham kh·∫£o**:
* [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) c·ªßa Junhao Zhao: Danh s√°ch c√°c b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c tuy·ªÉn ch·ªçn ƒë·ªÉ ƒë√†o t·∫°o tr∆∞·ªõc, tinh ch·ªânh v√† RLHF.
* [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt) c·ªßa Hugging Face:: Hu·∫•n luy·ªán tr∆∞·ªõc m√¥ h√¨nh GPT-2 t·ª´ ƒë·∫ßu b·∫±ng th∆∞ vi·ªán transformers.
* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM): Th∆∞ vi·ªán hi·ªán ƒë·∫°i ƒë·ªÉ hu·∫•n luy·ªán tr∆∞·ªõc c√°c m√¥ h√¨nh m·ªôt c√°ch hi·ªáu qu·∫£.
* [TinyLlama](https://github.com/jzhang38/TinyLlama) c·ªßa Zhang et al.: H√£y xem d·ª± √°n n√†y ƒë·ªÉ hi·ªÉu r√µ v·ªÅ c√°ch ƒë√†o t·∫°o m√¥ h√¨nh Llama ngay t·ª´ ƒë·∫ßu.
* [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) c·ªßa Hugging Face: Gi·∫£i th√≠ch s·ª± kh√°c bi·ªát gi·ªØa m√¥ h√¨nh h√≥a ng√¥n ng·ªØ nh√¢n qu·∫£ v√† ng√¥n ng·ªØ m·∫∑t n·∫° c≈©ng nh∆∞ c√°ch tinh ch·ªânh nhanh ch√≥ng m√¥ h√¨nh DistilGPT-2.
* [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) c·ªßa nostalgebraist: Th·∫£o lu·∫≠n v·ªÅ c√°c quy lu·∫≠t chia t·ª∑ l·ªá v√† gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa ch√∫ng ƒë·ªëi v·ªõi LLM n√≥i chung.
* [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4) c·ªßa BigScience: C√°c trang kh√°i ni·ªám m√¥ t·∫£ c√°ch x√¢y d·ª±ng m√¥ h√¨nh BLOOM, v·ªõi nhi·ªÅu th√¥ng tin h·ªØu √≠ch v·ªÅ ph·∫ßn k·ªπ thu·∫≠t v√† c√°c v·∫•n ƒë·ªÅ g·∫∑p ph·∫£i.
* [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) c·ªßa Meta: Nh·∫≠t k√Ω nghi√™n c·ª©u cho th·∫•y ƒëi·ªÅu g√¨ sai v√† ƒëi·ªÅu g√¨ ƒë√∫ng. H·ªØu √≠ch n·∫øu b·∫°n d·ª± ƒë·ªãnh ƒë√†o t·∫°o tr∆∞·ªõc m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ r·∫•t l·ªõn (trong tr∆∞·ªùng h·ª£p n√†y l√† tham s·ªë 175B).

---
### 4. Tinh ch·ªânh ƒë∆∞·ª£c gi√°m s√°t (Supervised Fine-Tuning)

C√°c m√¥ h√¨nh ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc ch·ªâ ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ nhi·ªám v·ª• d·ª± ƒëo√°n m√£ th√¥ng b√°o ti·∫øp theo, ƒë√≥ l√† l√Ω do t·∫°i sao ch√∫ng kh√¥ng ph·∫£i l√† tr·ª£ l√Ω h·ªØu √≠ch. SFT cho ph√©p b·∫°n ƒëi·ªÅu ch·ªânh ch√∫ng ƒë·ªÉ ƒë√°p ·ª©ng c√°c ch·ªâ d·∫´n d·∫´n. H∆°n n·ªØa, n√≥ cho ph√©p b·∫°n tinh ch·ªânh m√¥ h√¨nh c·ªßa m√¨nh tr√™n b·∫•t k·ª≥ d·ªØ li·ªáu n√†o (ri√™ng t∆∞, kh√¥ng b·ªã GPT-4 nh√¨n th·∫•y, v.v.) v√† s·ª≠ d·ª•ng n√≥ m√† kh√¥ng ph·∫£i tr·∫£ ti·ªÅn cho API nh∆∞ OpenAI.

* **Tinh ch·ªânh to√†n b·ªô (Full fine-tuning)**: Tinh ch·ªânh to√†n b·ªô ƒë·ªÅ c·∫≠p ƒë·∫øn vi·ªác hu·∫•n luy·ªán t·∫•t c·∫£ c√°c tham s·ªë trong m√¥ h√¨nh. ƒê√¢y kh√¥ng ph·∫£i l√† m·ªôt k·ªπ thu·∫≠t hi·ªáu qu·∫£ nh∆∞ng n√≥ mang l·∫°i k·∫øt qu·∫£ t·ªët h∆°n m·ªôt ch√∫t.
* [**LoRA**](https://arxiv.org/abs/2106.09685): M·ªôt k·ªπ thu·∫≠t hi·ªáu qu·∫£ v·ªÅ tham s·ªë (PEFT) d·ª±a tr√™n c√°c b·ªô ƒëi·ªÅu h·ª£p c·∫•p th·∫•p. Thay v√¨ ƒë√†o t·∫°o t·∫•t c·∫£ c√°c tham s·ªë, ch√∫ng t√¥i ch·ªâ ƒë√†o t·∫°o nh·ªØng b·ªô ƒëi·ªÅu h·ª£p n√†y.
* [**QLoRA**](https://arxiv.org/abs/2305.14314): M·ªôt PEFT kh√°c d·ª±a tr√™n LoRA, c≈©ng l∆∞·ª£ng t·ª≠ h√≥a tr·ªçng s·ªë c·ªßa m√¥ h√¨nh th√†nh 4 bit v√† gi·ªõi thi·ªáu c√°c tr√¨nh t·ªëi ∆∞u h√≥a ph√¢n trang ƒë·ªÉ qu·∫£n l√Ω m·ª©c tƒÉng ƒë·ªôt bi·∫øn c·ªßa b·ªô nh·ªõ.
* **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: M·ªôt c√¥ng c·ª• tinh ch·ªânh m·∫°nh m·∫Ω v√† th√¢n thi·ªán v·ªõi ng∆∞·ªùi d√πng, ƒë∆∞·ª£c s·ª≠ d·ª•ng trong nhi·ªÅu m√¥ h√¨nh ngu·ªìn m·ªü ti√™n ti·∫øn nh·∫•t.
* [**DeepSpeed**](https://www.deepspeed.ai/): ƒê√†o t·∫°o tr∆∞·ªõc v√† tinh ch·ªânh LLM hi·ªáu qu·∫£ cho c√†i ƒë·∫∑t nhi·ªÅu GPU v√† nhi·ªÅu n√∫t (ƒë∆∞·ª£c tri·ªÉn khai trong Axolotl).


üìö **T√†i li·ªáu tham kh·∫£o**:
* [The Novice's LLM Training Guide](https://rentry.org/llm-training) b·ªüi Alpin: T·ªïng quan v·ªÅ c√°c kh√°i ni·ªám v√† tham s·ªë ch√≠nh c·∫ßn xem x√©t khi tinh ch·ªânh LLM.
* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) b·ªüi Sebastian Raschka: Nh·ªØng hi·ªÉu bi·∫øt th·ª±c t·∫ø v·ªÅ LoRA v√† c√°ch ch·ªçn th√¥ng s·ªë t·ªët nh·∫•t.
* [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html): H∆∞·ªõng d·∫´n th·ª±c h√†nh v·ªÅ c√°ch tinh ch·ªânh m√¥ h√¨nh Llama 2 b·∫±ng th∆∞ vi·ªán Hugging Face.
* [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff) c·ªßa Benjamin Marie: C√°c ph∆∞∆°ng ph√°p hay nh·∫•t ƒë·ªÉ b·ªï sung c√°c v√≠ d·ª• ƒë√†o t·∫°o cho LLM nh√¢n qu·∫£
* [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html): H∆∞·ªõng d·∫´n c√°ch tinh ch·ªânh m√¥ h√¨nh CodeLlama b·∫±ng Axolotl.

---
### 5. H·ªçc tƒÉng c∆∞·ªùng t·ª´ ph·∫£n h·ªìi c·ªßa con ng∆∞·ªùi

Sau khi tinh ch·ªânh c√≥ gi√°m s√°t, RLHF l√† b∆∞·ªõc ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°c c√¢u tr·∫£ l·ªùi c·ªßa LLM ph√π h·ª£p v·ªõi mong ƒë·ª£i c·ªßa con ng∆∞·ªùi. √ù t∆∞·ªüng l√† t√¨m hi·ªÉu c√°c s·ªü th√≠ch t·ª´ ph·∫£n h·ªìi c·ªßa con ng∆∞·ªùi (ho·∫∑c nh√¢n t·∫°o), ph·∫£n h·ªìi n√†y c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£m b·ªõt th√†nh ki·∫øn, ki·ªÉm duy·ªát m√¥ h√¨nh ho·∫∑c khi·∫øn h·ªç h√†nh ƒë·ªông theo c√°ch h·ªØu √≠ch h∆°n. N√≥ ph·ª©c t·∫°p h∆°n SFT v√† th∆∞·ªùng ƒë∆∞·ª£c xem l√† t√πy ch·ªçn.

* **B·ªô d·ªØ li·ªáu ∆∞u ti√™n**: Nh·ªØng b·ªô d·ªØ li·ªáu n√†y th∆∞·ªùng ch·ª©a m·ªôt s·ªë c√¢u tr·∫£ l·ªùi v·ªõi m·ªôt s·ªë lo·∫°i x·∫øp h·∫°ng, ƒëi·ªÅu n√†y khi·∫øn ch√∫ng kh√≥ t·∫°o ra h∆°n c√°c b·ªô d·ªØ li·ªáu h∆∞·ªõng d·∫´n.
* [**T·ªëi ∆∞u h√≥a ch√≠nh s√°ch g·∫ßn nh·∫•t**](https://arxiv.org/abs/1707.06347): Thu·∫≠t to√°n n√†y t·∫≠n d·ª•ng m√¥ h√¨nh ph·∫ßn th∆∞·ªüng ƒë·ªÉ d·ª± ƒëo√°n li·ªáu m·ªôt vƒÉn b·∫£n nh·∫•t ƒë·ªãnh c√≥ ƒë∆∞·ª£c con ng∆∞·ªùi x·∫øp h·∫°ng cao hay kh√¥ng. D·ª± ƒëo√°n n√†y sau ƒë√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t·ªëi ∆∞u h√≥a m√¥ h√¨nh SFT v·ªõi m·ª©c ph·∫°t d·ª±a tr√™n s·ª± ph√¢n k·ª≥ KL.
* **[T·ªëi ∆∞u h√≥a t√πy ch·ªçn tr·ª±c ti·∫øp](https://arxiv.org/abs/2305.18290)**: DPO ƒë∆°n gi·∫£n h√≥a quy tr√¨nh b·∫±ng c√°ch s·∫Øp x·∫øp l·∫°i quy tr√¨nh th√†nh m·ªôt v·∫•n ƒë·ªÅ ph√¢n lo·∫°i. N√≥ s·ª≠ d·ª•ng m√¥ h√¨nh tham chi·∫øu thay v√¨ m√¥ h√¨nh ph·∫ßn th∆∞·ªüng (kh√¥ng c·∫ßn ƒë√†o t·∫°o) v√† ch·ªâ y√™u c·∫ßu m·ªôt si√™u tham s·ªë, gi√∫p n√≥ ·ªïn ƒë·ªãnh v√† hi·ªáu qu·∫£ h∆°n.

üìö **T√†i li·ªáu tham kh·∫£o**:
* [An Introduction to Training LLMs using RLHF](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) c·ªßa Ayush Thakur: Gi·∫£i th√≠ch t·∫°i sao RLHF l·∫°i ƒë∆∞·ª£c mong mu·ªën ƒë·ªÉ gi·∫£m sai l·ªách v√† tƒÉng hi·ªáu su·∫•t trong LLM.
* [Illustration RLHF](https://huggingface.co/blog/rlhf) c·ªßa Hugging Face: Gi·ªõi thi·ªáu v·ªÅ RLHF v·ªõi ƒë√†o t·∫°o m√¥ h√¨nh khen th∆∞·ªüng v√† tinh ch·ªânh b·∫±ng h·ªçc t·∫≠p tƒÉng c∆∞·ªùng.
* [StackLLaMA](https://huggingface.co/blog/stackllama) c·ªßa Hugging Face: H∆∞·ªõng d·∫´n cƒÉn ch·ªânh hi·ªáu qu·∫£ m√¥ h√¨nh LLaMA v·ªõi RLHF b·∫±ng th∆∞ vi·ªán transformers.
* [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) c·ªßa Hugging Face: H∆∞·ªõng d·∫´n tinh ch·ªânh m√¥ h√¨nh Llama 2 b·∫±ng DPO.
* [LLM Training: RLHF and Its Alternatives](https://substack.com/profile/27393275-sebastian-raschka-phd) c·ªßa Sebastian Rashcka: T·ªïng quan v·ªÅ quy tr√¨nh RLHF v√† c√°c l·ª±a ch·ªçn thay th·∫ø nh∆∞ RLAIF.

---
### 6. ƒê√°nh gi√° (Evaluation)

ƒê√°nh gi√° LLM l√† m·ªôt ph·∫ßn ƒë∆∞·ª£c ƒë√°nh gi√° th·∫•p trong quy tr√¨nh, t·ªën th·ªùi gian v√† c√≥ ƒë·ªô tin c·∫≠y v·ª´a ph·∫£i. Nhi·ªám v·ª• ti·∫øp theo c·ªßa b·∫°n ph·∫£i ch·ªâ ra nh·ªØng g√¨ b·∫°n mu·ªën ƒë√°nh gi√°, nh∆∞ng h√£y lu√¥n nh·ªõ ƒë·ªãnh lu·∫≠t Goodhart: "khi m·ªôt th∆∞·ªõc ƒëo tr·ªü th√†nh m·ª•c ti√™u, n√≥ kh√¥ng c√≤n l√† th∆∞·ªõc ƒëo t·ªët n·ªØa".
* **S·ªë li·ªáu truy·ªÅn th·ªëng**: C√°c s·ªë li·ªáu nh∆∞ m·ª©c ƒë·ªô b·ªëi r·ªëi v√† ƒëi·ªÉm BLEU kh√¥ng c√≤n ph·ªï bi·∫øn v√¨ ch√∫ng c√≥ sai s√≥t trong h·∫ßu h·∫øt c√°c b·ªëi c·∫£nh. ƒêi·ªÅu quan tr·ªçng v·∫´n l√† ph·∫£i hi·ªÉu ch√∫ng v√† khi n√†o ch√∫ng c√≥ th·ªÉ ƒë∆∞·ª£c √°p d·ª•ng.
* **ƒêi·ªÉm chu·∫©n chung**: D·ª±a tr√™n [Khai th√°c ƒë√°nh gi√° m√¥ h√¨nh ng√¥n ng·ªØ](https://github.com/EleutherAI/lm-evaluation-harness), [B·∫£ng x·∫øp h·∫°ng LLM m·ªü](https://huggingface.co/ Spaces/HuggingFaceH4/open_llm_leaderboard) l√† ƒëi·ªÉm chu·∫©n ch√≠nh cho c√°c LLM c√≥ m·ª•c ƒë√≠ch chung (nh∆∞ ChatGPT). C√≥ c√°c ƒëi·ªÉm chu·∫©n ph·ªï bi·∫øn kh√°c nh∆∞ [BigBench](https://github.com/google/BIG-bench), [MT-Bench](https://arxiv.org/abs/2306.05685), v.v.
* **ƒêi·ªÉm chu·∫©n d√†nh ri√™ng cho t·ª´ng nhi·ªám v·ª•**: C√°c nhi·ªám v·ª• nh∆∞ t√≥m t·∫Øt, d·ªãch thu·∫≠t, tr·∫£ l·ªùi c√¢u h·ªèi c√≥ ƒëi·ªÉm chu·∫©n, s·ªë li·ªáu chuy√™n d·ª•ng v√† th·∫≠m ch√≠ c·∫£ c√°c mi·ªÅn ph·ª• (y t·∫ø, t√†i ch√≠nh, v.v.), ch·∫≥ng h·∫°n nh∆∞ [PubMedQA](https://pubmedqa.github. io/) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi y sinh.
* **ƒê√°nh gi√° c·ªßa con ng∆∞·ªùi**: ƒê√°nh gi√° ƒë√°ng tin c·∫≠y nh·∫•t l√† t·ª∑ l·ªá ch·∫•p nh·∫≠n c·ªßa ng∆∞·ªùi d√πng ho·∫∑c so s√°nh do con ng∆∞·ªùi th·ª±c hi·ªán. N·∫øu b·∫°n mu·ªën bi·∫øt m·ªôt m√¥ h√¨nh c√≥ ho·∫°t ƒë·ªông t·ªët hay kh√¥ng, c√°ch ƒë∆°n gi·∫£n nh·∫•t nh∆∞ng ch·∫Øc ch·∫Øn nh·∫•t l√† b·∫°n h√£y t·ª± m√¨nh s·ª≠ d·ª•ng n√≥.

üìö **T√†i li·ªáu tham kh·∫£o**:
* [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity) c·ªßa Hugging Face: T·ªïng quan v·ªÅ s·ª± ph·ª©c t·∫°p c·ªßa m√£ ƒë·ªÉ tri·ªÉn khai n√≥ v·ªõi th∆∞ vi·ªán transformers.
* [BLEU at your own risk](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) c·ªßa Rachael Tatman: T·ªïng quan v·ªÅ ƒëi·ªÉm BLEU v√† nhi·ªÅu v·∫•n ƒë·ªÅ k√®m theo v√≠ d·ª•.
* [A Survey on Evaluation of LLMs](https://arxiv.org/abs/2307.03109) c·ªßa Chang et al.: B√†i vi·∫øt ƒë·∫ßy ƒë·ªß v·ªÅ nh·ªØng g√¨ c·∫ßn ƒë√°nh gi√°, ƒë√°nh gi√° ·ªü ƒë√¢u v√† ƒë√°nh gi√° nh∆∞ th·∫ø n√†o.
* [Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) c·ªßa lmsys: X·∫øp h·∫°ng Elo c·ªßa LLM c√≥ m·ª•c ƒë√≠ch chung, d·ª±a tr√™n s·ª± so s√°nh c·ªßa con ng∆∞·ªùi.

---
### 7. L∆∞·ª£ng t·ª≠ h√≥a (Quantization)

L∆∞·ª£ng t·ª≠ h√≥a l√† qu√° tr√¨nh chuy·ªÉn ƒë·ªïi tr·ªçng s·ªë (v√† k√≠ch ho·∫°t) c·ªßa m√¥ h√¨nh v·ªõi ƒë·ªô ch√≠nh x√°c th·∫•p h∆°n. V√≠ d·ª•: tr·ªçng s·ªë ƒë∆∞·ª£c l∆∞u tr·ªØ b·∫±ng 16 bit c√≥ th·ªÉ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh bi·ªÉu di·ªÖn 4 bit. K·ªπ thu·∫≠t n√†y ng√†y c√†ng tr·ªü n√™n quan tr·ªçng ƒë·ªÉ gi·∫£m chi ph√≠ t√≠nh to√°n v√† b·ªô nh·ªõ li√™n quan ƒë·∫øn LLM.

* **K·ªπ thu·∫≠t c∆° b·∫£n**: T√¨m hi·ªÉu c√°c m·ª©c ƒë·ªô ch√≠nh x√°c kh√°c nhau (FP32, FP16, INT8, v.v.) v√† c√°ch th·ª±c hi·ªán l∆∞·ª£ng t·ª≠ h√≥a ƒë∆°n gi·∫£n b·∫±ng k·ªπ thu·∫≠t absmax v√† ƒëi·ªÉm 0.
* **GGUF v√† llama.cpp**: Ban ƒë·∫ßu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ch·∫°y tr√™n CPU, [llama.cpp](https://github.com/ggerganov/llama.cpp) v√† ƒë·ªãnh d·∫°ng GGUF ƒë√£ tr·ªü th√†nh c√¥ng c·ª• ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ ch·∫°y LLM tr√™n ph·∫ßn c·ª©ng c·∫•p ƒë·ªô ng∆∞·ªùi ti√™u d√πng.
* **GPTQ v√† EXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) v√† c·ª• th·ªÉ h∆°n l√† ƒë·ªãnh d·∫°ng [EXL2](https://github.com/turboderp/exllamav2) cung c·∫•p t·ªëc ƒë·ªô ƒë√°ng kinh ng·∫°c nh∆∞ng ch·ªâ c√≥ th·ªÉ ch·∫°y tr√™n GPU. C√°c m√¥ h√¨nh c≈©ng m·∫•t nhi·ªÅu th·ªùi gian ƒë·ªÉ l∆∞·ª£ng t·ª≠ h√≥a.
* **AWQ**: ƒê·ªãnh d·∫°ng m·ªõi n√†y ch√≠nh x√°c h∆°n GPTQ (ƒë·ªô ph·ª©c t·∫°p th·∫•p h∆°n) nh∆∞ng s·ª≠ d·ª•ng nhi·ªÅu VRAM h∆°n v√† kh√¥ng nh·∫•t thi·∫øt ph·∫£i nhanh h∆°n.

üìö **T√†i li·ªáu tham kh·∫£o**:
* [Introduction to quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): T·ªïng quan v·ªÅ l∆∞·ª£ng t·ª≠ h√≥a, l∆∞·ª£ng t·ª≠ h√≥a absmax v√† ƒëi·ªÉm 0 c≈©ng nh∆∞ LLM.int8() b·∫±ng m√£.
* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): H∆∞·ªõng d·∫´n c√°ch l∆∞·ª£ng t·ª≠ h√≥a m√¥ h√¨nh Llama 2 b·∫±ng c√°ch s·ª≠ d·ª•ng llama.cpp v√† ƒë·ªãnh d·∫°ng GGUF.
* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): H∆∞·ªõng d·∫´n c√°ch ƒë·ªãnh l∆∞·ª£ng LLM b·∫±ng thu·∫≠t to√°n GPTQ v·ªõi AutoGPTQ.
* [ExLlamaV2: The Fastest Library to Run LLMs](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html): H∆∞·ªõng d·∫´n c√°ch l∆∞·ª£ng t·ª≠ h√≥a m√¥ h√¨nh Mistral b·∫±ng ƒë·ªãnh d·∫°ng EXL2 v√† ch·∫°y n√≥ v·ªõi th∆∞ vi·ªán ExLlamaV2.

---
### 8. T·ªëi ∆∞u h√≥a suy lu·∫≠n (Inference optimization)

* **Ch√∫ √Ω nhanh (flash attention)**: T·ªëi ∆∞u h√≥a c∆° ch·∫ø ch√∫ √Ω ƒë·ªÉ chuy·ªÉn ƒë·ªïi ƒë·ªô ph·ª©c t·∫°p c·ªßa n√≥ t·ª´ b·∫≠c hai sang tuy·∫øn t√≠nh, tƒÉng t·ªëc c·∫£ qu√° tr√¨nh hu·∫•n luy·ªán v√† suy lu·∫≠n.
* **B·ªô nh·ªõ ƒë·ªám kh√≥a-gi√° tr·ªã**: Hi·ªÉu b·ªô nh·ªõ ƒë·ªám kh√≥a-gi√° tr·ªã v√† c√°c c·∫£i ti·∫øn ƒë∆∞·ª£c gi·ªõi thi·ªáu trong [Ch√∫ √Ω nhi·ªÅu truy v·∫•n](https://arxiv.org/abs/1911.02150) (MQA) v√† [Ch√∫ √Ω truy v·∫•n theo nh√≥m] (https://arxiv.org/abs/2305.13245) (GQA).
* **Gi·∫£i m√£ suy ƒëo√°n**: S·ª≠ d·ª•ng m√¥ h√¨nh nh·ªè ƒë·ªÉ t·∫°o b·∫£n nh√°p, sau ƒë√≥ ƒë∆∞·ª£c m√¥ h√¨nh l·ªõn h∆°n xem x√©t ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô t·∫°o vƒÉn b·∫£n.
* **M√£ h√≥a v·ªã tr√≠**: Hi·ªÉu m√£ h√≥a v·ªã tr√≠ trong transfomers, ƒë·∫∑c bi·ªát l√† c√°c s∆° ƒë·ªì t∆∞∆°ng ƒë·ªëi nh∆∞ [RoPE](https://arxiv.org/abs/2104.09864), [ALiBi](https://arxiv.org/abs/2108.12409 ) v√† [YaRN](https://arxiv.org/abs/2309.00071). (Kh√¥ng ƒë∆∞·ª£c k·∫øt n·ªëi tr·ª±c ti·∫øp v·ªõi t·ªëi ∆∞u h√≥a suy lu·∫≠n m√† v·ªõi c√°c c·ª≠a s·ªï ng·ªØ c·∫£nh d√†i h∆°n.)

üìö **T√†i li·ªáu tham kh·∫£o**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) c·ªßa Hugging Face: Gi·∫£i th√≠ch c√°ch t·ªëi ∆∞u h√≥a suy lu·∫≠n tr√™n GPU.
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) c·ªßa Hugging Face: Gi·∫£i th√≠ch ba k·ªπ thu·∫≠t ch√≠nh ƒë·ªÉ t·ªëi ∆∞u h√≥a t·ªëc ƒë·ªô v√† b·ªô nh·ªõ, ƒë√≥ l√† l∆∞·ª£ng t·ª≠ h√≥a, Ch√∫ √Ω nhanh v√† ƒë·ªïi m·ªõi ki·∫øn tr√∫c.
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) c·ªßa Hugging Face: Phi√™n b·∫£n gi·∫£i m√£ suy ƒëo√°n c·ªßa HF, ƒë√¢y l√† m·ªôt b√†i ƒëƒÉng blog th√∫ v·ªã v·ªÅ c√°ch n√≥ ho·∫°t ƒë·ªông v·ªõi m√£ ƒë·ªÉ tri·ªÉn khai n√≥.
* [Extending the RoPE](https://blog.eleuther.ai/yarn/) c·ªßa EleutherAI: B√†i vi·∫øt t√≥m t·∫Øt c√°c k·ªπ thu·∫≠t m√£ h√≥a v·ªã tr√≠ kh√°c nhau.
* [Extending Context is Hard... but not Impossible](https://kaiokendev.github.io/context) c·ªßa kaiokendev: B√†i ƒëƒÉng tr√™n blog n√†y gi·ªõi thi·ªáu k·ªπ thu·∫≠t SuperHOT v√† cung c·∫•p m·ªôt b·∫£n kh·∫£o s√°t tuy·ªát v·ªùi v·ªÅ c√¥ng vi·ªác li√™n quan.

### L·ªùi c·∫£m ∆°n

Kh√≥a h·ªçc n√†y ƒë∆∞·ª£c d·ªãch t·ª´ [Maxime Labonne](https://github.com/mlabonne/llm-course)

Special thanks to Thomas Thelen for motivating me to create a roadmap, and Andr√© Frade for his input and review of the first draft.

*Disclaimer: I am not affiliated with any sources listed here.*

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date)](https://star-history.com/#mlabonne/llm-course&Date)
